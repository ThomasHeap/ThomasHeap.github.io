---
layout: archive
title: "Deep Learning Theory Reading Group"
permalink: /DLTRG/
author_profile: true
---

{% include base_path %}

## Schedule

| Date            | Paper                                                                                  | Resources  |
| --------------- |:--------------------------------------------------------------------------------------: |:----------:|
| 06/07/2022      | [Principles of Deep Learning Theory](https://deeplearningtheory.com/) Chapters 1 & 2                                      |  [Chapter 1](https://thomasheap.github.io/files/Pdfs/PoDLT/Chapter1.pdf) [Chapter 2](https://thomasheap.github.io/files/Pdfs/PoDLT/Chapter2.pdf)|
| 20/07/2022      | [PoDLT](https://deeplearningtheory.com/) Chapter 3                                                                        | [Chapter 3](https://thomasheap.github.io/files/Pdfs/PoDLT/Chapter3.pdf) [Chapter 4](https://thomasheap.github.io/files/Pdfs/PoDLT/Chapter4.pdf)|
| 27/07/2022      | We decided to change approach and read papers                                          |     |
| 03/08/2022      | [Neural Tangent Kernel](https://arxiv.org/abs/1806.07572)                                                                  | <https://rajatvd.github.io/NTK/>  [Notes](https://thomasheap.github.io/files/Pdfs/Neural_Tangent_Kernel.pdf)|
| 17/08/2022      | [Multilayer Feedforward Networks are Universal Approximators](https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf)                           | [Notes](https://thomasheap.github.io/files/Pdfs/Multilayer_Feedforward_Networks_are_Universal_Approximators.pdf)  |
| 31/08/2022      | [Explaining Neural Scaling Laws](https://arxiv.org/abs/2102.06701)                                                         |   |
| 22/09/2022      | [Git Re-Basin](https://arxiv.org/abs/2209.04836)                                                        |   |
| 05/10/2022      | [Monte Carlo Gradient Estimation in Machine Learning](https://arxiv.org/abs/1906.10652)    Section 4             |[Notes](https://thomasheap.github.io/files/Pdfs/Monte_Carlo_Gradient_Estimation_in_Machine_Learning.pdf)    |
| 12/10/2022      | [Monte Carlo Gradient Estimation in Machine Learning](https://arxiv.org/abs/1906.10652) Section 5                 |[Notes](https://thomasheap.github.io/files/Pdfs/Monte_Carlo_Gradient_Estimation_in_Machine_Learning.pdf)    |
| 19/10/2022      | [Monte Carlo Gradient Estimation in Machine Learning](https://arxiv.org/abs/1906.10652) Section 7...                |[Notes](https://thomasheap.github.io/files/Pdfs/Monte_Carlo_Gradient_Estimation_in_Machine_Learning.pdf)    |
| 26/10/2022      | [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) Section 1,2 & 3                 |[Notes](https://thomasheap.github.io/files/Pdfs/ToyModelsofSuperposition.pdf)    |
| 16/11/2022      | [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) Section 4,5 & 6  | |
| 23/11/2022      | [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) Section 7,8 & 9,10  | |
| 30/11/2022      | [Gradient Estimation with Discrete Stein Operators](https://openreview.net/forum?id=I1mkUkaguP)  | |
| 07/12/2022      | [Exact learning dynamics of deep linear networks with prior knowledge](https://openreview.net/forum?id=lJx2vng-KiC)  | |
| 04/01/2023      | [Neural networks and physical systems with emergent collective computational abilities (Hopfield Networks)](https://authors.library.caltech.edu/7427/1/HOPpnas82.pdf)  |[Notes](https://thomasheap.github.io/files/Pdfs/hopfield_networks.pdf)|
| 11/01/2023      | [CSC2541 Winter 2022 Topics in Machine Learning: Neural Net Training Dynamics](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L01_intro.pdf) Lecture 1 ||
| 18/01/2023      | [CSC2541 Winter 2022 Topics in Machine Learning: Neural Net Training Dynamics](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L02_Taylor_approximations.pdf) Lecture 2 ||
| 11/01/2023      | [CSC2541 Winter 2022 Topics in Machine Learning: Neural Net Training Dynamics](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L03_metrics.pdf) Lecture 3 ||
| 25/01/2023      | [CSC2541 Winter 2022 Topics in Machine Learning: Neural Net Training Dynamics](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L04_second_order.pdf) Lecture 4 ||
| 01/02/2023      | [CSC2541 Winter 2022 Topics in Machine Learning: Neural Net Training Dynamics](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L05_normalization.pdf) Lecture 5 ||
| 15/03/2023      | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs](https://arxiv.org/abs/2303.00848) ||
| 22/03/2023      | [Laplace Redux -- Effortless Bayesian Deep Learning](https://arxiv.org/abs/2106.14806)||
| 29/03/2023      | [A Theory on Adam Instability in Large-Scale Machine Learning](https://arxiv.org/pdf/2304.09871.pdf)||
| 12/04/2023      | [Sigma-Reparam: Stable Transformer Training with Spectral Reparametrization](https://openreview.net/forum?id=QwqxO8URJzn)||
| 24/05/2023      | [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)||
| 31/05/2023      | [Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent ](https://openreview.net/pdf?id=QC10RmRbZy9)||
| 7/06/2023      | [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://arxiv.org/abs/2305.14342)||
| 14/06/2023      | [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)||
|09/08/2023        | [Limitations of the Empirical Fisher Approximation for Natural Gradient Descent](https://arxiv.org/abs/1905.12558)||
| 23/08/2023      | [The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning](https://arxiv.org/abs/2304.05366)||
| 20/09/2023      | [Transformers as Support Vector Machines](https://arxiv.org/abs/2308.16898)||
| 27/09/2023      | [Flat Minima](https://www.bioinf.jku.at/publications/older/3304.pdf)||
| 04/10/2023      | [Language Modeling Is Compression](https://arxiv.org/abs/2309.10668) ||
| 11/10/2023      | [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) ||
| 18/10/2023      | [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512) ||
